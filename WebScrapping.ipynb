{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOllzSurvqOBCrEY6lL0lUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ERIC10000/API/blob/main/WebScrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "1_301jnUCmfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvGDTMTj5zBt"
      },
      "outputs": [],
      "source": [
        "# Web Scrapping: Technique in Data Science of Extracting Information from the websites available on the internet\n",
        "# Digital Economy: Most of organizations, business, journal, research papers are readily available\n",
        "# on the internet\n",
        "# As Data Scientist, we need a way to extract these valuable from the internet, perform analysis and\n",
        "# get the insights\n",
        "# Requirement: Websites are made of tags(<openingTag>Content</closingTag>)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5Np8eeZElUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTfnpC7XEyFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https:/-import-ban-on-apple-watches-amid-masimo-patent-battle\"\n",
        "page = requests.get(URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "div = soup.findAll(\"div\", class_ = \"dw-content-box-p\")\n",
        "soup = BeautifulSoup(str(div))\n",
        "text = soup.get_text()\n",
        "print(text)\n",
        "text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_hHYFbkoFTnI",
        "outputId": "5c3657ad-0649-4043-ef68-ce0470b4b3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ste1: import nltk(Natural Language Processing Took) -> python library\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDZRmEteHZYf",
        "outputId": "68814102-6c7f-41cf-9a8f-5ea45679f51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sentences or words, or phrases -> Token\n",
        "# Tokenization:\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "tokenized_text = word_tokenize(text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_fd2T7ZHpkM",
        "outputId": "51c2f64c-8d6b-48a9-dd94-ce49bbe609c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Stop Words:\n",
        "# Stops Words: Connecting words in sentences that dont carry meaning context\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVuuuMu7H8Ir",
        "outputId": "5011568a-33d1-4cdc-dc70-2478a71149de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the fileids -> Different Languages\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.fileids())\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "print(english_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOdp-NgzIA7R",
        "outputId": "6111dd25-c3c6-4d98-bc85-5bf5b152cf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
            "{'mustn', 'which', 'doing', \"won't\", 'been', \"you'll\", 'ours', 'very', 'then', \"wasn't\", 'again', 'wasn', 'more', 'down', \"doesn't\", \"that'll\", 'her', 'below', \"she's\", 'couldn', 'further', 'themselves', \"you're\", 'its', 'of', 'myself', 'did', 'each', \"shan't\", 'from', 'into', 'won', \"wouldn't\", \"you'd\", \"didn't\", \"haven't\", 'be', 'ourselves', 'for', 'any', 'shan', 'his', \"aren't\", 'few', 'at', 'this', \"it's\", 'am', 'that', 'same', 'will', \"isn't\", 'who', 'own', 'aren', 'herself', 've', 'both', 'off', 'during', 'does', 'hers', 'those', 'needn', 'between', \"you've\", 'once', 'them', \"needn't\", 'were', 'other', 'such', 're', 'shouldn', 'm', 'whom', 'over', 'if', 'or', 'they', 'through', 'he', 'had', 'can', 'theirs', 'i', 'to', \"mightn't\", 'their', 'in', 'are', 'having', 'why', \"don't\", 'yours', 'yourselves', 'me', 'being', 'should', 'no', \"shouldn't\", 'about', 'mightn', 'himself', 'your', 'is', 'while', 'by', 'isn', 'before', 'there', 'yourself', 'has', 'we', 'not', 'now', 'until', 'o', 'after', \"mustn't\", 'when', 'and', 'some', 'weren', 'wouldn', 'have', 'under', 'here', 'our', \"should've\", 'above', 's', 'ma', 'him', 'my', 't', 'don', 'out', 'do', 'nor', 'you', 'itself', 'doesn', 'didn', 'because', 'it', 'with', 'y', 'd', 'she', 'an', 'where', \"weren't\", 'll', 'was', 'haven', 'most', 'too', 'how', 'hasn', 'these', 'hadn', 'all', 'but', 'only', 'up', 'so', 'as', 'what', 'than', 'the', 'against', 'ain', \"couldn't\", \"hadn't\", 'just', \"hasn't\", 'a', 'on'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "meanful_words = []\n",
        "for word in tokenized_text:\n",
        "    if word.lower() not in english_stopwords:\n",
        "        meanful_words.append(word)\n",
        "# meanful_word: contains all words without stopwords\n",
        "print(meanful_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwcFTodUIHYf",
        "outputId": "2d8f53a0-2281-424c-a27e-37828a8c2c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the Words: By Removing all the punctuations e,g ,.!...\n",
        "clean_words = []\n",
        "for word in meanful_words:\n",
        "    if word.isalpha():\n",
        "        clean_words.append(word)\n",
        "\n",
        "print(clean_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd3e2VqqIMla",
        "outputId": "b2945d06-a8b1-43de-93a6-7de3e060a09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution: Statistical representation of how often different values or elements occur in a dataset.\n",
        "# Goal:To summarize and understand the distribution of data.\n",
        "from nltk.probability import FreqDist\n",
        "frequency_count = FreqDist(clean_words)\n",
        "frequency_count.most_common(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHchxbsaIRBh",
        "outputId": "e8ea47e4-51ce-4d29-b076-bc76d1d123c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the FreqDist on Wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image\n",
        "\n",
        "# create a function to generate WordCloud\n",
        "def generate_wordcloud(text):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    wc = WordCloud(background_color=\"white\", max_words=1000, stopwords=stopwords, repeat=True)\n",
        "    wc.generate(str(text))\n",
        "    wc.to_file(\"wc.png\")\n",
        "    path = \"wc.png\"\n",
        "    display(Image.open(path))"
      ],
      "metadata": {
        "id": "elmYIlkQI0Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud(frequency_count.most_common(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "GzChjtzZI5HY",
        "outputId": "4d87cb91-2bc9-47f7-9599-a969cc8c3621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-9a4917d6c1a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequency_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-a3272f6a8d7a>\u001b[0m in \u001b[0;36mgenerate_wordcloud\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wc.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"wc.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    412\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
          ]
        }
      ]
    }
  ]
}